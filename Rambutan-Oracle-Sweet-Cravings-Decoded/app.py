{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install faiss-cpu\npip install ipywidgets\npip install streamlit\npip install sentence-transformers\npip install plotly\npip install pandas\npip install tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport plotly.express as px\nimport streamlit as st\n\n# Load and clean datasets\n@st.cache\ndef load_data():\n    data_en = pd.read_csv(\"cleaned_Konsumsi_Rambutan_Perkapita_2024_en.csv\", delimiter=\",\")\n    data_id = pd.read_csv(\"cleaned_Konsumsi_Rambutan_Perkapita_2024_id.csv\", delimiter=\",\")\n\n    # Fix combined columns\n    if len(data_en.columns) == 1:\n        data_en[['Region', 'Consumption']] = data_en.iloc[:, 0].str.split(',', expand=True)\n    if len(data_id.columns) == 1:\n        data_id[['Region', 'Consumption']] = data_id.iloc[:, 0].str.split(',', expand=True)\n\n    # Convert \"Consumption\" to numeric values\n    data_en[\"Consumption\"] = pd.to_numeric(data_en[\"Consumption\"], errors='coerce').fillna(0)\n    data_id[\"Consumption\"] = pd.to_numeric(data_id[\"Consumption\"], errors='coerce').fillna(0)\n\n    # Concatenate datasets\n    data = pd.concat([data_en, data_id], ignore_index=True)\n\n    # Clean dataset\n    data = data.drop_duplicates(subset=[\"Region\", \"Consumption\"])\n    data = data.dropna(subset=[\"Region\", \"Consumption\"])\n    data[\"Region\"] = data[\"Region\"].str.strip()\n\n    return data\n\ndata = load_data()\n\n# Generate embeddings\n@st.cache(allow_output_mutation=True)\ndef generate_embeddings(data):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    batch_size = 32\n    embeddings = []\n\n    for i in tqdm(range(0, len(data), batch_size), desc=\"Generating Embeddings\"):\n        batch_data = data[\"Region\"].iloc[i:i+batch_size].tolist()\n        batch_embeddings = model.encode(batch_data)\n        embeddings.extend(batch_embeddings)\n\n    embeddings_np = np.array(embeddings)\n    return embeddings_np, model\n\nembeddings_np, model = generate_embeddings(data)\n\n# Build FAISS index\n@st.cache(allow_output_mutation=True)\ndef create_faiss_index(embeddings_np):\n    dimension = embeddings_np.shape[1]\n    index = faiss.IndexFlatL2(dimension)\n    index.add(embeddings_np)\n    return index\n\nindex = create_faiss_index(embeddings_np)\n\n# Function to query FAISS index\ndef search_region(query, top_k=5):\n    query_embedding = model.encode([query.strip()])[0]\n    distances, indices = index.search(np.array([query_embedding]), top_k)\n    results = data.iloc[indices[0]]\n    results = results[[\"Region\", \"Consumption\"]]\n    return results\n\n# Streamlit App Layout\nst.title(\"Rambutan Consumption Analysis\")\nst.write(\"Explore rambutan consumption data across regions.\")\n\n# Input for search query and top-k results\nregion_query = st.text_input(\"Enter Region Name:\")\ntop_k = st.slider(\"Number of Results\", 1, 10, 5)\n\n# Search and display results\nif region_query:\n    results = search_region(region_query, top_k)\n    if not results.empty:\n        st.write(results)\n        \n        # Visualize results\n        fig_query = px.bar(\n            results,\n            x=\"Region\",\n            y=\"Consumption\",\n            title=f\"Top {top_k} Results for Query: {region_query}\",\n            labels={\"Region\": \"Region\", \"Consumption\": \"Consumption (tons)\"},\n            color=\"Consumption\"\n        )\n        st.plotly_chart(fig_query)\n    else:\n        st.write(\"Region not found!\")\n\n# Display full dataset chart\nst.header(\"Full Dataset Visualization\")\nfig = px.bar(\n    data,\n    x=\"Region\",\n    y=\"Consumption\",\n    title=\"Rambutan Consumption Across Regions\",\n    labels={\"Region\": \"Region\", \"Consumption\": \"Consumption (tons)\"},\n    color=\"Consumption\",\n    height=600,\n    width=1500\n)\nfig.update_layout(\n    xaxis=dict(\n        tickangle=-45,\n        automargin=True,\n        title=dict(standoff=10),\n        showgrid=False,\n        type=\"category\",\n        rangeslider=dict(visible=True)\n    )\n)\nst.plotly_chart(fig)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}