{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11310113,"sourceType":"datasetVersion","datasetId":7055438}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:46:04.756789Z","iopub.execute_input":"2025-04-08T15:46:04.757412Z","iopub.status.idle":"2025-04-08T15:46:05.308543Z","shell.execute_reply.started":"2025-04-08T15:46:04.757362Z","shell.execute_reply":"2025-04-08T15:46:05.306531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu\n!pip install ipywidgets\n#!pip install streamlit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:46:05.310266Z","iopub.execute_input":"2025-04-08T15:46:05.310948Z","iopub.status.idle":"2025-04-08T15:46:15.038382Z","shell.execute_reply.started":"2025-04-08T15:46:05.310905Z","shell.execute_reply":"2025-04-08T15:46:15.036953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport plotly.express as px\nimport faiss\n#import streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm  # For progress tracking\nfrom ipywidgets import interact","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:46:15.040978Z","iopub.execute_input":"2025-04-08T15:46:15.041338Z","iopub.status.idle":"2025-04-08T15:46:25.008599Z","shell.execute_reply.started":"2025-04-08T15:46:15.041309Z","shell.execute_reply":"2025-04-08T15:46:25.007219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(df.columns) #check column name\n\n# Step 1: Load and clean both of the dataset\ndata_en = pd.read_csv(\"/kaggle/input/produksi-rambutan-sul-sel-2019-2020/cleaned_Konsumsi_Rambutan_Perkapita_2024_en.csv\", delimiter=\",\")\ndata_id = pd.read_csv(\"/kaggle/input/produksi-rambutan-sul-sel-2019-2020/cleaned_Konsumsi_Rambutan_Perkapita_2024_id.csv\", delimiter=\",\")\n\n# Step 3: Fix combined columns (Split if necessary)\nif len(data_en.columns) == 1:\n    data_en[['Region', 'Consumption']] = data_en.iloc[:, 0].str.split(',', expand=True)\nif len(data_id.columns) == 1:\n    data_id[['Region', 'Consumption']] = data_id.iloc[:, 0].str.split(',', expand=True)\n\n# Convert \"Consumption\" to numeric values\ndata_en[\"Consumption\"] = pd.to_numeric(data_en[\"Consumption\"], errors='coerce').fillna(0)\ndata_id[\"Consumption\"] = pd.to_numeric(data_id[\"Consumption\"], errors='coerce').fillna(0)\n\n# Concatenate both datasets into a unified DataFrame\ndata = pd.concat([data_en, data_id], ignore_index=True)\n\n# Clean dataset: Remove duplicates and handle missing values\ndata = data.drop_duplicates(subset=[\"Region\", \"Consumption\"])  # Remove duplicate rows based on key columns\ndata = data.dropna(subset=[\"Region\", \"Consumption\"])  # Remove rows with missing critical values\ndata[\"Region\"] = data[\"Region\"].str.strip()  # Strip whitespace from region names\n\nprint(f\"Cleaned dataset loaded successfully with {len(data)} rows.\")\nprint(\"Columns in the dataset:\", data.columns)\n\n# Step 4: Generate embeddings in batches for efficiency\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nbatch_size = 32\nembeddings = []\n\n# Process embeddings in batches\nfor i in tqdm(range(0, len(data), batch_size), desc=\"Generating Embeddings\"):\n    batch_data = data[\"Region\"].iloc[i:i+batch_size].tolist()\n    batch_embeddings = model.encode(batch_data)\n    embeddings.extend(batch_embeddings)\n\n# Convert to NumPy array for FAISS\nembeddings_np = np.array(embeddings)\n\n# Step 5: Build FAISS index\ndimension = embeddings_np.shape[1]\nindex = faiss.IndexFlatL2(dimension)  # Use L2 distance for similarity\nindex.add(embeddings_np)  # Add embeddings to FAISS index\n\nprint(f\"FAISS index created successfully with {index.ntotal} entries.\")\n\n# Function to query FAISS index\ndef search_region(query, top_k=5):\n    query_embedding = model.encode([query.strip()])[0]  # Encode query and strip whitespace\n    distances, indices = index.search(np.array([query_embedding]), top_k)  # Search FAISS index\n    results = data.iloc[indices[0]]  # Retrieve matching rows\n    \n    # Keep only relevant columns\n    results = results[[\"Region\", \"Consumption\"]]\n    \n    return results\n\n\n# Step 6: Interactive querying and result visualization\ndef search_and_display(region, top_k=5):\n    results = search_region(region, top_k)\n\n    if not results.empty:\n        print(\"\\nSearch Results:\")\n        print(results)\n\n        # Generate bar chart\n        fig_query = px.bar(\n            results,\n            x=\"Region\",\n            y=\"Consumption\",\n            title=f\"Top {top_k} Results for Query: {region}\",\n            labels={\"Region\": \"Region\", \"Consumption\": \"Consumption (tons)\"},\n            color=\"Consumption\",\n            height=400,\n            width=800\n        )\n        fig_query.show()\n    else:\n        print(\"\\nRegion not found!\")\n\n# Step 7: Create a wide bar chart with both horizontal scrolling and range slider\nfig = px.bar(\n    data,\n    x=\"Region\",\n    y=\"Consumption\",\n    title=\"Rambutan Consumption Across Regions (English & Bahasa Indonesia)\",\n    labels={\"Region\": \"Region\", \"Consumption\": \"Consumption (tons)\"},\n    color=\"Consumption\",\n    height=600,\n    width=1500  # Extend chart width for horizontal scrolling\n)\n\n# Enhance layout with scroll features\nfig.update_layout(\n    xaxis=dict(\n        tickangle=-45,\n        automargin=True,\n        title=dict(standoff=10),\n        showgrid=False,\n        type=\"category\",\n        rangeslider=dict(visible=True)  # Enable the range slider\n    )\n)\n\n# Display the chart\nfig.show()\n\n# Step 8: Add ipywidgets for interactivity in Jupyter\ninteract(search_and_display, \n         region=data[\"Region\"].unique(),  # Dropdown for unique regions\n         top_k=(1, 10, 1))  # Slider for top_k range\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:00:19.324801Z","iopub.execute_input":"2025-04-08T16:00:19.325288Z","iopub.status.idle":"2025-04-08T16:00:23.502430Z","shell.execute_reply.started":"2025-04-08T16:00:19.325247Z","shell.execute_reply":"2025-04-08T16:00:23.501259Z"}},"outputs":[],"execution_count":null}]}